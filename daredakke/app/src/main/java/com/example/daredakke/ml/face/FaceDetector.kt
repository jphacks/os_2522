package com.example.daredakke.ml.face

import android.content.Context
import android.graphics.Bitmap
import androidx.camera.core.ExperimentalGetImage
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetectorOptions
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import androidx.compose.ui.geometry.Rect
import com.example.daredakke.constants.AppConstants
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.launch
import kotlinx.coroutines.withContext
import com.example.daredakke.data.repository.PersonRepository
import com.example.daredakke.utils.FaceImageStorage
import kotlin.math.roundToInt

/**
 * ML Kit ã‚’ä½¿ç”¨ã—ãŸé¡”æ¤œå‡ºã‚¯ãƒ©ã‚¹
 * Phase 1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¡”æ¤œå‡ºã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
 * Phase 2: é¡”åŸ‹ã‚è¾¼ã¿æŠ½å‡ºã¨èªè­˜æ©Ÿèƒ½ã‚’çµ±åˆ
 * Phase 4: å†ä¼šæ™‚ã®è¦ç´„è¡¨ç¤ºæ©Ÿèƒ½
 */
class FaceDetector(
    private val appContext: Context,
    private val embeddingExtractor: FaceEmbeddingExtractor? = null,
    private val faceRecognizer: FaceRecognizer? = null,
    private val personRepository: PersonRepository? = null
) : ImageAnalysis.Analyzer {
    
    private val faceDetectorOptions = FaceDetectorOptions.Builder()
        .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_FAST)
        .setLandmarkMode(FaceDetectorOptions.LANDMARK_MODE_ALL)
        .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_NONE)
        .setMinFaceSize(0.1f) // ç”»åƒã®10%ä»¥ä¸Šã®ã‚µã‚¤ã‚ºã®é¡”ã‚’æ¤œå‡º
        .enableTracking()
        .build()
    
    private val detector = FaceDetection.getClient(faceDetectorOptions)
    
    private val _detectionResults = MutableStateFlow<List<FaceDetectionResult>>(emptyList())
    val detectionResults: StateFlow<List<FaceDetectionResult>> = _detectionResults.asStateFlow()
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æƒ…å ±ã‚’ä¿æŒ
    private val trackingInfoMap = mutableMapOf<Int, FaceTrackingInfo>()
    private var unknownIdCounter = 1
    
    // åŸ‹ã‚è¾¼ã¿æŠ½å‡ºã®é–“éš”åˆ¶å¾¡
    private val lastEmbeddingExtractionTime = mutableMapOf<Int, Long>()
    
    // æœ€å¾Œã«æŠ½å‡ºã—ãŸåŸ‹ã‚è¾¼ã¿ã‚’ä¿å­˜
    private val lastExtractedEmbeddings = mutableMapOf<Int, FloatArray>()
    
    // æœ€å¾Œã«åˆ‡ã‚Šå‡ºã—ãŸé¡”ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä¿å­˜
    private val lastCapturedFaceBitmaps = mutableMapOf<Int, Bitmap>()
    
    
    // åº§æ¨™å¤‰æ›ç”¨ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ã‚º
    private var previewWidth: Int = 0
    private var previewHeight: Int = 0
    private var isUsingFrontCamera: Boolean = true
    

     
    // ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ä¸­ãªã©ä¿æŒã™ã¹ã trackingId ã‚’ãƒ”ãƒ³ç•™ã‚
    private val pinnedTrackingIds = mutableSetOf<Int>()

    fun pinTrackingId(trackingId: Int) {
        pinnedTrackingIds.add(trackingId)
    }

    fun unpinTrackingId(trackingId: Int) {
        pinnedTrackingIds.remove(trackingId)
    }


    // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šåº§æ¨™å¤‰æ›ãƒ¢ãƒ¼ãƒ‰
    private enum class CoordinateTransformMode {
        RAW,        // ç”Ÿåº§æ¨™ï¼ˆå¤‰æ›ãªã—ï¼‰
        SIMPLE,     // ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        COMPLEX     // è¤‡é›‘ãªå¤‰æ›ï¼ˆå›è»¢ãƒ»åè»¢å¯¾å¿œï¼‰
    }
    private val transformMode = CoordinateTransformMode.SIMPLE // ãƒ†ã‚¹ãƒˆç”¨
    
    /**
     * ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã‚’è¨­å®šï¼ˆåº§æ¨™å¤‰æ›ç”¨ï¼‰
     */
    fun setPreviewSize(width: Int, height: Int) {
        previewWidth = width
        previewHeight = height
    }
    
    /**
     * ã‚«ãƒ¡ãƒ©ã®å‘ãã‚’è¨­å®š
     */
    fun setCameraFacing(isFrontCamera: Boolean) {
        isUsingFrontCamera = isFrontCamera
    }
    
    /**
     * ã‚·ãƒ³ãƒ—ãƒ«ãªåº§æ¨™å¤‰æ›ï¼ˆã‚µã‚¤ã‚ºãƒ»ä½ç½®èª¿æ•´ä»˜ãï¼‰
     */
    private fun transformCoordinatesSimple(
        bbox: android.graphics.Rect,
        imageWidth: Int,
        imageHeight: Int
    ): Rect {
        // ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        val displayWidth = if (previewWidth > 0) previewWidth.toFloat() else AppConstants.CAMERA_WIDTH.toFloat()
        val displayHeight = if (previewHeight > 0) previewHeight.toFloat() else AppConstants.CAMERA_HEIGHT.toFloat()
        
        // åŸºæœ¬çš„ãªã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        val scaleX = displayWidth / imageWidth.toFloat()
        val scaleY = displayHeight / imageHeight.toFloat()
        
        // å…ƒã®BBOXåº§æ¨™ã‚’ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        val originalLeft = bbox.left * scaleX
        val originalTop = bbox.top * scaleY
        val originalRight = bbox.right * scaleX
        val originalBottom = bbox.bottom * scaleY
        
        // å…ƒã®ã‚µã‚¤ã‚º
        val originalWidth = originalRight - originalLeft
        val originalHeight = originalBottom - originalTop
        
        // èª¿æ•´å¾Œã®ã‚µã‚¤ã‚º
        val adjustedWidth = originalWidth * 1.3f  // æ¨ªå¹…1.3å€
        val adjustedHeight = originalHeight        // é«˜ã•ã¯ãã®ã¾ã¾
        
        // èª¿æ•´å¾Œã®ä½ç½®ï¼ˆä¸­å¿ƒã‚’åŸºæº–ã«æ¨ªå¹…ã‚’æ‹¡å¤§ã€ä¸Šã«100pxç§»å‹•ï¼‰
        val centerX = (originalLeft + originalRight) / 2f
        val adjustedLeft = centerX - (adjustedWidth / 2f)
        val adjustedRight = centerX + (adjustedWidth / 2f)
        val adjustedTop = originalTop - 100f  // 100pxä¸Šã«ç§»å‹•
        val adjustedBottom = adjustedTop + adjustedHeight
        
        val result = if (isUsingFrontCamera) {
            // ãƒ•ãƒ­ãƒ³ãƒˆã‚«ãƒ¡ãƒ©ã®å ´åˆï¼šå·¦å³åè»¢
            Rect(
                left = displayWidth - adjustedRight,
                top = adjustedTop,
                right = displayWidth - adjustedLeft,
                bottom = adjustedBottom
            )
        } else {
            // ã‚¢ã‚¦ãƒˆã‚«ãƒ¡ãƒ©ã®å ´åˆï¼šé€šå¸¸ã®å¤‰æ›
            Rect(
                left = adjustedLeft,
                top = adjustedTop,
                right = adjustedRight,
                bottom = adjustedBottom
            )
        }
        
        println("BBOX adjustment: scale=${scaleX}x${scaleY}, frontCam=$isUsingFrontCamera")
        println("Original: w=${originalWidth.toInt()}, h=${originalHeight.toInt()}")
        println("Adjusted: w=${adjustedWidth.toInt()}, h=${adjustedHeight.toInt()}, moved up 100px")
        println("Result: $result")
        return result
    }
    
    /**
     * ã€ä¿ç•™ã€‘è¤‡é›‘ãªåº§æ¨™å¤‰æ›ï¼ˆå¾Œã§ä½¿ç”¨ï¼‰
     */
    private fun transformCoordinatesComplex(
        bbox: android.graphics.Rect,
        imageWidth: Int,
        imageHeight: Int,
        rotationDegrees: Int
    ): Rect {
        // è¤‡é›‘ãªå¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¾Œã§å®Ÿè£…ï¼‰
        return Rect(
            left = bbox.left.toFloat(),
            top = bbox.top.toFloat(),
            right = bbox.right.toFloat(),
            bottom = bbox.bottom.toFloat()
        )
    }

    @androidx.annotation.OptIn(ExperimentalGetImage::class)
    override fun analyze(imageProxy: ImageProxy) {
        val inputImage = InputImage.fromMediaImage(
            imageProxy.image!!,
            imageProxy.imageInfo.rotationDegrees
        )
        
        detector.process(inputImage)
            .addOnSuccessListener { faces ->
                val currentTime = System.currentTimeMillis()
                val results = mutableListOf<FaceDetectionResult>()
                
                // ç¾åœ¨æ¤œå‡ºã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã®ã‚»ãƒƒãƒˆ
                val currentTrackingIds = mutableSetOf<Int>()
                
                faces.forEach { face ->
                    val trackingId = face.trackingId
                    if (trackingId != null) {
                        currentTrackingIds.add(trackingId)
                        
                        // åº§æ¨™å¤‰æ›ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ‡ã‚Šæ›¿ãˆ
                        val boundingBox = when (transformMode) {
                            CoordinateTransformMode.RAW -> {
                                // ç”Ÿåº§æ¨™ï¼ˆå¤‰æ›ãªã—ï¼‰
                                Rect(
                                    left = face.boundingBox.left.toFloat(),
                                    top = face.boundingBox.top.toFloat(),
                                    right = face.boundingBox.right.toFloat(),
                                    bottom = face.boundingBox.bottom.toFloat()
                                )
                            }
                            CoordinateTransformMode.SIMPLE -> {
                                // ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
                                transformCoordinatesSimple(
                                    face.boundingBox,
                                    imageProxy.width,
                                    imageProxy.height
                                )
                            }
                            CoordinateTransformMode.COMPLEX -> {
                                // è¤‡é›‘ãªå¤‰æ›
                                transformCoordinatesComplex(
                                    face.boundingBox,
                                    imageProxy.width,
                                    imageProxy.height,
                                    imageProxy.imageInfo.rotationDegrees
                                )
                            }
                        }
                        
                        // ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
                        println("=== BBOX Debug ===")
                        println("Transform mode: $transformMode")
                        println("Face detected: trackingId=$trackingId")
                        println("Original ML Kit bbox: ${face.boundingBox}")
                        println("Transformed bbox: $boundingBox")
                        println("Image size: ${imageProxy.width}x${imageProxy.height}")
                        println("Preview size: ${previewWidth}x${previewHeight}")
                        println("Rotation: ${imageProxy.imageInfo.rotationDegrees}Â°")
                        
                        // ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æƒ…å ±ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
                        val trackingInfo = trackingInfoMap.getOrPut(trackingId) {
                            FaceTrackingInfo(trackingId = trackingId)
                        }
                        
                        // å®‰å®šæ€§ã‚’æ›´æ–°
                        val centerPoint = Pair(boundingBox.center.x, boundingBox.center.y)
                        val minSize = minOf(boundingBox.width, boundingBox.height)
                        val isStable = trackingInfo.updateStability(centerPoint, minSize, currentTime)
                        
                        // Phase 2: å®‰å®šã—ãŸé¡”ã«å¯¾ã—ã¦åŸ‹ã‚è¾¼ã¿æŠ½å‡ºã¨èªè­˜å‡¦ç†
                        if (isStable && embeddingExtractor != null && faceRecognizer != null) {
                            processStableFaceForRecognition(face, trackingId, trackingInfo, imageProxy)
                        }
                        
                        // Unknown IDã®å‰²ã‚Šå½“ã¦ï¼ˆèªè­˜ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
                        // å®‰å®šã—ãŸé¡”ã®ã¿å‡¦ç†ã—ã€èªè­˜å‡¦ç†ä¸­ã¯å¾…æ©Ÿã—ã¦ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã‚’å›é¿
                        if (isStable &&
                            trackingInfo.recognizedPersonId == null &&
                            trackingInfo.unknownId == null &&
                            trackingInfo.hasAttemptedRecognition &&
                            !trackingInfo.isRecognitionInProgress) {
                            trackingInfo.unknownId = unknownIdCounter++
                            println("ğŸ†” Assigned UnknownId #${trackingInfo.unknownId} to trackingId=$trackingId")
                        }
                        
                        // èªè­˜æƒ…å ±ã®ä½œæˆï¼ˆå®‰å®šã—ãŸé¡”ã®ã¿ï¼‰
                        val recognitionInfo = if (isStable) {
                            createRecognitionInfo(trackingInfo)
                        } else {
                            null // ä¸å®‰å®šãªé¡”ã«ã¯èªè­˜æƒ…å ±ã‚’è¡¨ç¤ºã—ãªã„
                        }

                        val result = FaceDetectionResult(
                            face = face,
                            boundingBox = boundingBox,
                            trackingId = trackingId,
                            isStable = isStable,
                            confidence = 1.0f, // ML Kitã¯ä¿¡é ¼åº¦ã‚’ç›´æ¥æä¾›ã—ãªã„ãŸã‚å›ºå®šå€¤
                            recognitionInfo = recognitionInfo
                        )
                        
                        results.add(result)
                    }
                }
                
                // æ¤œå‡ºã•ã‚Œãªããªã£ãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã‚’å‰Šé™¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ï¼‰
                val expiredTrackingIds = trackingInfoMap.keys.filter { id ->
                    id !in currentTrackingIds && 
                    currentTime - trackingInfoMap[id]!!.lastDetectionTime > 5000L // 5ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                }
                expiredTrackingIds
                .filterNot { it in pinnedTrackingIds } // è¿½åŠ : ãƒ”ãƒ³ç•™ã‚ã¯æƒé™¤ã—ãªã„
                .forEach { expiredId ->
                    trackingInfoMap.remove(expiredId)
                    lastEmbeddingExtractionTime.remove(expiredId)
                    lastExtractedEmbeddings.remove(expiredId)
                    clearFaceThumbnail(expiredId)
                }
                
                _detectionResults.value = results
            }
            .addOnFailureListener { exception ->
                // ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆå®Ÿéš›ã®ã‚¢ãƒ—ãƒªã§ã¯ãƒ­ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ï¼‰
                println("Face detection failed: ${exception.message}")
                _detectionResults.value = emptyList()
            }
            .addOnCompleteListener {
                imageProxy.close()
            }
    }
    
    /**
     * æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã®é¡”ã‚’æ—¢çŸ¥ã®äººç‰©ã¨ã—ã¦èªè­˜
     */
    fun recognizeFace(trackingId: Int, personId: Long) {
        trackingInfoMap[trackingId]?.let { trackingInfo ->
            trackingInfo.recognizedPersonId = personId
            trackingInfo.unknownId = null
        }
    }
    
    /**
     * æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã®Unknown IDã‚’å–å¾—
     */
    fun getUnknownId(trackingId: Int): Int? {
        return trackingInfoMap[trackingId]?.unknownId
    }
    
    /**
     * æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°IDã®èªè­˜æ¸ˆã¿äººç‰©IDã‚’å–å¾—
     */
    fun getRecognizedPersonId(trackingId: Int): Long? {
        return trackingInfoMap[trackingId]?.recognizedPersonId
    }
    
    /**
     * ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
     */
    fun clearTrackingInfo() {
        trackingInfoMap.clear()
        unknownIdCounter = 1
        lastEmbeddingExtractionTime.clear()
        lastExtractedEmbeddings.clear()
        clearAllFaceThumbnails()
    }
    
    /**
     * èªè­˜æƒ…å ±ã‚’ä½œæˆ
     */
    private fun createRecognitionInfo(trackingInfo: FaceTrackingInfo): FaceRecognitionInfo {
        return if (trackingInfo.recognizedPersonId != null) {
            // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰äººç‰©æƒ…å ±ã¨è¦ç´„ã‚’å–å¾—
            val cachedInfo = trackingInfo.cachedPersonInfo
            
            FaceRecognitionInfo(
                personId = trackingInfo.recognizedPersonId,
                personName = cachedInfo?.personName ?: "èªè­˜æ¸ˆã¿",
                unknownId = null,
                confidence = 0.9f, // ä»®ã®ä¿¡é ¼åº¦
                lastSummary = cachedInfo?.lastSummary // Phase 4: è¦ç´„è¡¨ç¤º
            )
        } else {
            FaceRecognitionInfo(
                personId = null,
                personName = null,
                unknownId = trackingInfo.unknownId,
                confidence = 0f
            )
        }
    }
    
    /**
     * é¡”ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
     */
    private fun cacheFaceThumbnail(trackingId: Int, bitmap: Bitmap) {
        lastCapturedFaceBitmaps.remove(trackingId)?.recycle()
        lastCapturedFaceBitmaps[trackingId] = bitmap
    }
    
    /**
     * é¡”ã‚µãƒ ãƒã‚¤ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
     */
    private fun clearFaceThumbnail(trackingId: Int) {
        lastCapturedFaceBitmaps.remove(trackingId)?.recycle()
    }
    
    /**
     * ã™ã¹ã¦ã®é¡”ã‚µãƒ ãƒã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
     */
    private fun clearAllFaceThumbnails() {
        lastCapturedFaceBitmaps.forEach { (_, bitmap) ->
            bitmap.recycle()
        }
        lastCapturedFaceBitmaps.clear()
    }
    
    /**
     * é¡”ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ç”Ÿæˆ
     */
    private fun captureFaceThumbnail(
        frameBitmap: Bitmap,
        face: com.google.mlkit.vision.face.Face
    ): Bitmap? {
        val bbox = face.boundingBox
        if (bbox.width() <= 0 || bbox.height() <= 0) {
            return null
        }
        
        val widthScale = 1.3f
        val heightScale = 1.35f
        val centerX = bbox.exactCenterX()
        val centerY = bbox.exactCenterY() - bbox.height() * 0.1f
        
        val halfWidth = (bbox.width() * widthScale / 2f)
        val halfHeight = (bbox.height() * heightScale / 2f)
        
        val left = (centerX - halfWidth).roundToInt().coerceAtLeast(0)
        val top = (centerY - halfHeight).roundToInt().coerceAtLeast(0)
        val right = (centerX + halfWidth).roundToInt().coerceAtMost(frameBitmap.width)
        val bottom = (centerY + halfHeight).roundToInt().coerceAtMost(frameBitmap.height)
        
        val finalWidth = right - left
        val finalHeight = bottom - top
        
        if (finalWidth <= 0 || finalHeight <= 0) {
            return null
        }
        
        return try {
            Bitmap.createBitmap(frameBitmap, left, top, finalWidth, finalHeight)
        } catch (e: Exception) {
            println("Failed to crop face thumbnail: ${e.message}")
            null
        }
    }
    
    
    /**
     * é¡”ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦äººç‰©ã®ä»£è¡¨ç”»åƒã«è¨­å®š
     */
    private suspend fun persistFaceThumbnailIfAvailable(personId: Long, trackingId: Int) {
        val bitmap = lastCapturedFaceBitmaps.remove(trackingId) ?: return
        try {
            val imagePath = withContext(Dispatchers.IO) {
                FaceImageStorage.saveFaceThumbnail(appContext, personId, bitmap)
            }
            if (imagePath != null) {
                personRepository?.updatePersonProfileImage(personId, imagePath)
            }
        } catch (e: Exception) {
            println("Failed to persist face thumbnail: ${e.message}")
        } finally {
            bitmap.recycle()
        }
    }
    
    /**
     * æ—¢çŸ¥äººç‰©ã«ä»£è¡¨ç”»åƒãŒç™»éŒ²ã•ã‚Œã¦ã„ãªã‘ã‚Œã°ç™»éŒ²ã™ã‚‹
     */
    private suspend fun ensureProfileImageForPerson(personId: Long, trackingId: Int) {
        val repository = personRepository ?: run {
            clearFaceThumbnail(trackingId)
            return
        }
        
        try {
            val person = repository.getPersonById(personId)
            if (person?.profileImagePath.isNullOrBlank()) {
                persistFaceThumbnailIfAvailable(personId, trackingId)
            } else {
                clearFaceThumbnail(trackingId)
            }
        } catch (e: Exception) {
            println("Failed to ensure profile image: ${e.message}")
            clearFaceThumbnail(trackingId)
        }
    }
    
    /**
     * å®‰å®šã—ãŸé¡”ã«å¯¾ã™ã‚‹èªè­˜å‡¦ç†
     */// ...existing code...
private fun processStableFaceForRecognition(
    face: com.google.mlkit.vision.face.Face,
    trackingId: Int,
    trackingInfo: FaceTrackingInfo,
    imageProxy: ImageProxy
) {
    val currentTime = System.currentTimeMillis()
    val lastTime = lastEmbeddingExtractionTime[trackingId] ?: 0L

    // æŠ½å‡ºé–“éš”ã¨æ—¢çŸ¥ã‚¬ãƒ¼ãƒ‰
    if (currentTime - lastTime < AppConstants.EMBEDDING_CAPTURE_INTERVAL_MS) return
    if (trackingInfo.recognizedPersonId != null) return

    // èªè­˜å‡¦ç†ä¸­ãƒ•ãƒ©ã‚°ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ›´æ–°
    trackingInfo.isRecognitionInProgress = true
    lastEmbeddingExtractionTime[trackingId] = currentTime

    // ImageProxy â†’ Bitmapï¼ˆã“ã“ã§åˆã‚ã¦ bitmap ã‚’ä½œã‚‹ï¼‰
    val bitmap = try {
        imageProxyToBitmap(imageProxy)
    } catch (e: Exception) {
        println("âŒ Failed to convert imageProxy to bitmap: ${e.message}")
        trackingInfo.isRecognitionInProgress = false
        return
    }

    // éåŒæœŸã§å‡¦ç†
    CoroutineScope(Dispatchers.IO).launch {
        try {
            // é¡”ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            captureFaceThumbnail(bitmap, face)?.let { thumbnail ->
                cacheFaceThumbnail(trackingId, thumbnail)
            }

            // åŸ‹ã‚è¾¼ã¿æŠ½å‡ºï¼ˆ1å›ã ã‘ï¼‰
            val embedding = embeddingExtractor?.extractEmbedding(bitmap, face)
            if (embedding == null) {
                CoroutineScope(Dispatchers.Main).launch {
                    trackingInfo.hasAttemptedRecognition = true
                    trackingInfo.isRecognitionInProgress = false
                    println("âš ï¸ Failed to extract embedding for trackingId=$trackingId")
                }
                return@launch
            }

            // ç›´è¿‘ã®åŸ‹ã‚è¾¼ã¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæ–°è¦ä¿å­˜ã«ä½¿ã†ï¼‰
            lastExtractedEmbeddings[trackingId] = embedding

            // èªè­˜
            val recognitionResult = faceRecognizer?.recognizeFace(embedding)
            println("ğŸ” Recognition result for trackingId=$trackingId: $recognitionResult")

            CoroutineScope(Dispatchers.Main).launch {
                trackingInfo.hasAttemptedRecognition = true
                trackingInfo.isRecognitionInProgress = false
                when (recognitionResult) {
                    is RecognitionResult.Recognized -> {
                        trackingInfo.recognizedPersonId = recognitionResult.personId
                        trackingInfo.unknownId = null
                        println("âœ… Recognized as personId=${recognitionResult.personId}, confidence=${recognitionResult.confidence}")

                        // æ—¢çŸ¥äººç‰©ã®å¾Œå‡¦ç†
                        CoroutineScope(Dispatchers.IO).launch {
                            try {
                                faceRecognizer?.addEmbeddingToExistingPerson(recognitionResult.personId, embedding)
                                ensureProfileImageForPerson(recognitionResult.personId, trackingId)
                                val (person, lastSummary) =
                                    personRepository?.getPersonWithLatestSummary(recognitionResult.personId) ?: Pair(null, null)
                                trackingInfo.cachedPersonInfo = CachedPersonInfo(
                                    personName = person?.name,
                                    lastSummary = lastSummary
                                )
                            } catch (e: Exception) {
                                println("Failed to cache person info: ${e.message}")
                            }
                        }
                    }
                    is RecognitionResult.Unknown, null -> {
                        println("â“ Not recognized (similarity below threshold)")
                    }
                }
            }
        } catch (e: Exception) {
            println("âŒ Failed to process face for recognition: ${e.message}")
            CoroutineScope(Dispatchers.Main).launch {
                trackingInfo.hasAttemptedRecognition = true
                trackingInfo.isRecognitionInProgress = false
            }
        }
    }
}
// ...existing code...
    
    /**
     * ImageProxyã‹ã‚‰Bitmapã‚’ä½œæˆï¼ˆYUV_420_888 â†’ RGBå¤‰æ›ï¼‰
     */
    private fun imageProxyToBitmap(imageProxy: ImageProxy): Bitmap {
        val yBuffer = imageProxy.planes[0].buffer
        val uBuffer = imageProxy.planes[1].buffer
        val vBuffer = imageProxy.planes[2].buffer
        
        val ySize = yBuffer.remaining()
        val uSize = uBuffer.remaining()
        val vSize = vBuffer.remaining()
        
        val nv21 = ByteArray(ySize + uSize + vSize)
        
        // Yæˆåˆ†ã‚’ã‚³ãƒ”ãƒ¼
        yBuffer.get(nv21, 0, ySize)
        
        // UVæˆåˆ†ã‚’ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ã—ã¦ã‚³ãƒ”ãƒ¼
        val uvPixelStride = imageProxy.planes[1].pixelStride
        if (uvPixelStride == 1) {
            // UVæˆåˆ†ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆ
            uBuffer.get(nv21, ySize, uSize)
            vBuffer.get(nv21, ySize + uSize, vSize)
        } else {
            // UVæˆåˆ†ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆ
            val uvBuffer = ByteArray(uSize + vSize)
            uBuffer.get(uvBuffer, 0, uSize)
            vBuffer.get(uvBuffer, uSize, vSize)
            
            var uvIndex = 0
            for (i in 0 until uSize step uvPixelStride) {
                nv21[ySize + uvIndex] = uvBuffer[i]
                nv21[ySize + uvIndex + 1] = uvBuffer[uSize + i]
                uvIndex += 2
            }
        }
        
        // YUV â†’ RGBå¤‰æ›
        return yuvToRgbBitmap(nv21, imageProxy.width, imageProxy.height)
    }
    
    /**
     * YUVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰RGB Bitmapã‚’ä½œæˆ
     */
    private fun yuvToRgbBitmap(yuv: ByteArray, width: Int, height: Int): Bitmap {
        val pixels = IntArray(width * height)
        val frameSize = width * height
        
        for (j in 0 until height) {
            for (i in 0 until width) {
                val y = yuv[j * width + i].toInt() and 0xFF
                val uvIndex = frameSize + (j / 2) * width + (i and 1.inv())
                val u = yuv[uvIndex].toInt() and 0xFF
                val v = yuv[uvIndex + 1].toInt() and 0xFF
                
                // YUV â†’ RGBå¤‰æ›å¼
                val r = (y + 1.402 * (v - 128)).toInt().coerceIn(0, 255)
                val g = (y - 0.344 * (u - 128) - 0.714 * (v - 128)).toInt().coerceIn(0, 255)
                val b = (y + 1.772 * (u - 128)).toInt().coerceIn(0, 255)
                
                pixels[j * width + i] = (0xFF shl 24) or (r shl 16) or (g shl 8) or b
            }
        }
        
        return Bitmap.createBitmap(pixels, width, height, Bitmap.Config.ARGB_8888)
    }
    
    /**
     * æ–°ã—ã„äººç‰©ã¨ã—ã¦ä¿å­˜
     */


    suspend fun saveNewPersonWithEmbedding(trackingId: Int, name: String): Long? {
        val embedding = lastExtractedEmbeddings[trackingId]
        if (embedding == null) {
            println("saveNewPersonWithEmbedding: no embedding for trackingId=$trackingId")
            return null
        }

        // ï¼ˆä»»æ„ï¼‰æ—¢å­˜ã¨è¿‘ã™ãã‚‹å ´åˆã¯æ–°è¦ä½œæˆã‚’é¿ã‘ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ãªã‚‰ã“ã“ã§

        val personId = faceRecognizer?.saveNewPerson(name, embedding)
        if (personId == null) {
            println("saveNewPersonWithEmbedding: repository returned null")
            return null
        }
        recognizeFace(trackingId, personId)

        // ä»£è¡¨ç”»åƒã®ä¿å­˜ï¼ˆä»»æ„ï¼‰
        CoroutineScope(Dispatchers.IO).launch {
            ensureProfileImageForPerson(personId, trackingId)
        }

        println("saveNewPersonWithEmbedding: created personId=$personId for trackingId=$trackingId")
        return personId
    }
    
    /**
     * ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾
     */
    fun release() {
        detector.close()
        trackingInfoMap.clear()
        lastEmbeddingExtractionTime.clear()
        lastExtractedEmbeddings.clear()
        clearAllFaceThumbnails()
        embeddingExtractor?.release()
    }
}
